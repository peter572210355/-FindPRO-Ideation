Tier 2：人機互動評估（衡量品質）

這一層是衡量真正推薦品質的關鍵，也是上線前產生更可靠「金標」的方法。
分為兩部分，分別為請專家對新計劃評級以對已知審查名單題目做評級

方法：
對一組真實研究計畫產生 Top-N 推薦清單。
組成領域專家小組（理想為真實或前任召集人）。
向專家呈現計畫與匿名化的推薦名單（不顯示分數或排名）。
請專家依相關度評分（例如：1 = 無關、2 = 牽強、3 = 相關、4 = 高度相關）。
好處：
產生高品質標註資料：專家評分可成為更可靠的金標。
支援更合適的指標：有了分級的相關性分數後，可使用 NDCG@K（歸一化折損累積增益），能獎勵把高度相關項目排在前面，符合我們需求。
定性回饋：可收集專家對評分原因的質性意見，補足量化指標的不足。
注意：這不是純量化任務，人的判斷很重要。我會協助一起執行人機評估。
# 專家評級流程與演算法評估方案

此文件說明如何利用多位專家對新題目的評級結果，來檢驗並更新我們的演算法排名。  
專家評級尺度為 **1=不重要、2=相關、3=重要、4=高度相關**。 （思考是否需要專家無法判斷值為空值） 
我們的演算法在每題會先產生一個候選人排名，展示 **前 k=15 名**供專家挑選。

---

---

## Step 2. 專家一致性檢定 (Inter-rater Reliability)

目的：確認不同專家對候選人的評級是否具有一致性，確保後續的共識標籤可信。  

常用指標：  

- **Krippendorff’s α（ordinal α）**  
  - 基於觀察分歧 (observed disagreement) 與期望分歧 (expected disagreement) 的比率。  
  - α = 1 完全一致；α = 0 等於隨機；α < 0 比隨機還差。  
  - 常用標準：α ≥ 0.67 視為可接受。  
  - **適用情境**：有序等級（例如 1–4）、能處理缺值。  

---

### 為什麼選這個指標？
	1.	最符合 ordinal 1–4 評級的資料特性（差距大小會被考慮）。
	2.	能處理缺值，比 κ 更實務。
	3.	學界常見標準，有較好說服力。

**公式核心：**

$$
\alpha = 1 - \frac{D_o}{D_e}
$$

- $D_o$：觀察到的分歧 (observed disagreement)  
- $D_e$：隨機情況下的期望分歧 (expected disagreement)  

在 **有序尺度 (ordinal scale)** 下，分歧的度量方式為：  

$$
\delta(c, c') = (c - c')^2
$$

也就是說，評級差距越大，懲罰越嚴重。  

---

**例子：**

假設有 2 位專家對 3 個候選人進行 1–4 分級：  

| 候選人 | 專家 A | 專家 B |
|--------|--------|--------|
| X      |   2    |   3    |
| Y      |   1    |   4    |
| Z      |   3    |   3    |

1. 計算每對標註的分歧：  
   - 候選人 X：$\delta(2,3) = (2-3)^2 = 1$
   - 候選人 Y：$\delta(1,4) = (1-4)^2 = 9$  
   - 候選人 Z：$\delta(3,3) = (3-3)^2 = 0$

   所以觀察到的平均分歧：  
   $$D_o = \frac{1+9+0}{3} = \frac{10}{3} \approx 3.33$$

2. 若評分是隨機的（期望分歧 $D_e$），假設計算得到 $D_e = 6$。  

3. 代入公式：  
   $$\alpha = 1 - \frac{D_o}{D_e} 
          = 1 - \frac{3.33}{6} 
          = 1 - 0.555 
          = 0.445$$

---

**解讀：**  
Krippendorff’s α = 0.445，代表專家之間有一定共識，但仍有分歧。  
若 α ≥ 0.67 通常被視為可接受的一致性。
---
## Step 3. 生成共識標籤 (Consensus Labeling)
若一致性足夠（無一致性跳到step5），可將專家評級整合成共識：  
- **平均分數法**：取所有專家分數平均，作為候選人分數。  
- **中位數 / 眾數**：避免極端值影響。  
- **加權平均**：依專家可靠度給權重。  

再依照分數高低轉換成 **共識排名**。

---

## Step 4. 系統排名 vs 專家共識比較
### Rank Correlation（秩相關）
- **Kendall’s τ-b**  
- 衡量兩個排名的一致性，能處理 ties。  
- 範圍 −1 ~ 1，值越高代表排序越一致。  
- **Somers’ D**  
- 指定方向的秩相關（例如「系統名次能否預測專家等級」）。  
- 與 AUC (Area Under Curve) 緊密關聯。  

### Trend Test（趨勢檢定）
- **Jonckheere–Terpstra test**  
- 檢定專家等級是否隨系統排名單調遞減。  

### Information Retrieval (IR) 指標
- **Precision@k**：前 k 名中相關候選比例。  
- **Recall@k**：前 k 名覆蓋了多少已知相關候選。    
- **MAP (Mean Average Precision)**：多個正例時的平均精度。  
- **MRR (Mean Reciprocal Rank)**：首個正例的倒數排名平均。  
- **NDCG (Normalized Discounted Cumulative Gain)**  
- 適合有序等級（1–4），可用增益值映射 (如 0/1/3/7)。  
- 強調「重要的候選應該排在更前面」。  

### 統計方法（可思考有無必要）
- **置換檢定 (Permutation Test)**：檢查系統指標是否顯著優於隨機排序。  

---

## Step 5. 若專家間不一致

當一致性指標（如 κ/α/W）偏低時，不建議直接產生單一共識標籤。以下三種路線可擇一或搭配使用。

---

### 5.1 逐專家比較（Per-expert evaluation + Wilcoxon 彙總）

**目的**  
在「沒有可靠共識」時，尊重每位專家的觀點，評估系統是否**整體傾向**與專家一致。

**做法**  
1. 對**每位**專家 `e`，計算「系統排名 vs 專家評級/排名」的秩相關（**Kendall’s τ-b** 或 **Somers’ D**）。  
2. 得到一組效應量：`{τ_e}` 或 `{D_e}`。  
3. 對 `{τ_e}`（或 `{D_e}`）做 **單尾 Wilcoxon 符號等級檢定**：  
   - H₀：中位數 = 0（整體無一致）  
   - H₁：中位數 > 0（整體呈正相關）  
4. 報告效應量（平均/中位數 ± CI）與 p 值；必要時再分層（題型、領域）。

**何時用**  
- 專家分歧大，**不宜硬做共識**。  
- 想回答「整體上，系統是否跟多數專家同向？」。

**優缺點**  
- ✅ 不強迫合一、統計上簡潔；能揭示「多數傾向」。  
- ⚠️ 仍需解釋離群專家（可做 leave-one-rater-out 檢視）。
### 5.2 仲裁 / 二輪標註（Adjudication / Second-round labeling）

**目的**  
透過溝通/校準，將「爭議樣本」聚焦處理，提升標註一致性與可用性。

**做法**  
1. **篩選爭議樣本**：如分散度高（分數方差/IQR 大）、評級分裂（如 1 與 4 並存）。  
2. **校準會議**：回顧標註指南與正反例，釐清定義與判準。  
3. **仲裁**：由資深專家或小組對爭議樣本給出最終標註（可保留備註理由）。  
4. **可選**：發佈「第二版指南」，再進行**二輪標註**以驗證改進（再算 κ/α）。

**何時用**  
- 需要**產出可落地的單一共識標籤**（供線上系統或訓練使用）。  
- 專家可投入少量時間開會或二次標註。

**優缺點**  
- ✅ 能矯正指南歧義，提升資料品質；實務可行。  
- ⚠️ 耗時；仍可能保留主觀性（建議記錄理由與決策依據）。
---
## NOTE : 專家不一致時的「逐專家比較」(step5) vs 專家一致時的「共識比較」(step2~4)
### 核心差異（TL;DR）
- **共識比較（專家一致時）**：把多位專家的評級合成**一套「地面實況」(ground truth)**，再檢定**系統 vs 這套地面實況**是否一致。
- **逐專家比較（專家不一致時）**：保留**每位專家的個別觀點**，先算**系統 vs 各專家**的相關係數，再用 **Wilcoxon** 檢定「**多數專家**是否與系統**整體同向**」。

### 對照表

| 面向 | 共識比較（一致性高時用） | 逐專家比較（一致性低時用） |
|---|---|---|
| **輸入標籤** | 已**聚合**成單一共識（平均/中位/加權/Borda…） | **未聚合**，保留每位專家的原始評級/排序 |
| **分析單位** | 以**題目**為單位算指標（NDCG@k、τ-b、Somers’ D），再彙總/CI | 以**專家**為單位得到一組相關值 {τ\_e 或 D\_e} |
| **虛無假設 H₀** | 系統與**共識標籤**無關（例如平均 NDCG 無法區分、或 τ-b=0） | 這組 {τ\_e}（或 {D\_e}）的**中位數 = 0**（整體無正向趨勢） |
| **檢定方法** | 置換檢定（對 NDCG/τ-b）、或對題目層差值做 Wilcoxon/Bootstrap CI | **Wilcoxon 符號等級**（單尾，檢中位數>0） |
| **回答的問題** | 「**系統是否和**『一套被認可的**共識真值**』**一致**？」 | 「**在多數專家眼裡**，系統是否**傾向**與他們一致？」（即使少數專家不同意） |
| **什麼時候用** | κ/α/W 顯示**一致性高**（例如 α≥0.67） | κ/α/W 顯示**一致性低/中**，不宜硬做共識 |
| **風險/侷限** | 若其實有兩個專家派系，硬聚合會**稀釋/扭曲**立場 | 少數離群專家會被「多數決」**壓過**，但保留了異質性 |

### 為何聽起來像「都在檢一致」？
- 名義上都在問「系統與專家是否一致」，但**主詞不同**：  
  - **共識比較**：系統 vs **單一共識**（把專家們視為「一個聲音」）。  
  - **逐專家比較**：系統 vs **每位專家**（承認有多個聲音），最後問「**多數**是否支持系統」（Wilcoxon）。

### 兩者可能得出不同結論的例子（直觀）
- 有 3 位專家：A、B 與系統高度一致（τ≈0.6），C 幾乎反向（τ≈-0.2）。  
- **逐專家比較**：{0.6, 0.55, -0.2} → Wilcoxon 仍可能顯著「中位數>0」，結論：**多數專家支持系統**。  
- **若硬做共識**：A/B 與 C 的觀點互相抵銷，合成標籤變得「溫和/模糊」，系統 vs 共識的 τ 可能只有 ~0.2 → **不顯著**。  
→ 說明兩條路線**回答的是不同問題**：一條問「對單一真值的符合度」，一條問「是否得到多數專家的支持」。

### 實務決策建議
1. **先算一致性（κ/α/W）**：  
   - **高** → 走「共識比較」。  
   - **低/中** → 走「逐專家比較（+ Wilcoxon）」；必要時做仲裁/二輪或 DS/MACE。  
2. **透明度**：即便走共識，也可附帶「逐專家結果分佈」當附錄，避免共識掩蓋爭議。
---



---

## Step 6. 解讀
- 若所有前 15 候選人均被評為「相關 (≥2)」 → 系統能有效產生高品質候選集。  
- 此時 Precision@15 幾乎為 100%，重點轉移到：  
- 系統是否能將「高度相關 (4)」者排在更前面？  
- 評估宜使用 **NDCG@k、Somers’ D、Kendall’s τ-b** 等排序敏感指標。
- 如果皆系統與專家皆不一致，要知道專家排名理由、演算法核心是否需要修改，找出不一致理由。
🔑 可以再做的事
---



# Part B：已知審查委員題目評估流程

本部分說明如何利用「已知審查委員」的題目，檢驗系統在既有案例上的表現。  
流程分為：資料準備 → 專家標註 → 一致性檢定 → 系統 vs 專家比較 → 錯誤診斷與回饋。

---

## Step 1. 資料準備
- 每題 $q$ 有候選委員集合，系統給出完整排名 $r_{qi}$（1 = 最前）。  
- 總候選人數 $N_q$。  
- 定義已知委員集合 $G_q$（這些是我們特別關注的人）。  

---

## Step 2. 專家評級 (Expert Annotation)
- **Top-k 名單**（例：前 15）：全部給專家評級  
  $g_{qi}\in\{1,2,3,4\}$ （1 = 不重要，4 = 高度相關）。  
- **已知委員**：無論是否在 Top-k，都要額外給專家評級。  
- **其他 k 外樣本**：不評級（標為 NA）。  

👉 這樣可確保「Top-k 的排序品質」與「已知委員是否被漏排」都能被檢查。  

---

## Step 3. 專家一致性檢定
確認多位專家是否對同一候選人評分一致。  

- **Fleiss’ κ（加權 κ）**：多專家類別一致性。  
- **Krippendorff’s α（ordinal α）**：最推薦，可處理 ordinal 尺度與缺值。  
- **Kendall’s W**：若專家提供排名。  

路線決策：  
- 一致性高 → 合成共識標籤（平均 / 中位 / 加權）。  
- 一致性低 → 保留逐專家標註，逐專家 vs 系統比較（Wilcoxon 彙總）。  

---

## Step 4. 系統 vs 專家比較

### (A) 整體一致性（主要目標）
檢查系統排序是否與專家評級整體一致。  

- **秩相關**  
  - Kendall’s $\tau_b$  
  - Somers’ $D$  

- **排序品質指標（IR 指標）**  
  - NDCG@K（僅 Top-k）  
  - **Pooled-NDCG**（Top-k + 已知委員）：

    ${pDCG}_(q)=sum_{i in L_q}\frac{G(g_{qi})}{log_2(r_{qi}+1)}$
    

    ${iDCG}_(q)=sum_{j=1}^{|L_q|}\frac{G\big(g_{q(j)}^{\downarrow}\big)}{log_2(j+1)}$  

    ${pNDCG}(q)=\frac{{pDCG}(q)}{{iDCG}(q)}$  

  - Precision@K / Recall@K  
  - MAP / MRR  

- **統計檢定**  
  - Permutation Test：檢查系統是否顯著優於隨機。  
  - Bootstrap CI（by query）：估計 $\tau_b$、NDCG 的區間。  

---

### (B) 已知委員補充診斷（次要，但關鍵）
專門針對 $G_q$ 做診斷分析。  

- **Coverage@K**  
  ${Coverage@}K(q)=\frac{|G_q\cap {TopK}_q|}{|G_q|}$  

- **HighRel-in-K**  
  已知委員進到 Top-k 中，多少被專家標為「重要以上」（$g\ge 3$）。  

- **Missed-Known@K**  
  已知委員在 k 外，但專家評高分 → 系統漏排。  

- **Rank-Gap / Percentile**  
  ${Gap}_{qi} = r_{qi}-K,\quad \text{Pct}_{qi} = 1-\frac{r_{qi}-1}{N_q}$  

- **Exposure-Weighted Recall (EWR)**  

  ${EWR}_(q)=\frac{\sum_{i\in L_q^{+}} {disc}(r_{qi})}{\sum_{j=1}^{|L_q^{+}|} {disc}(j)},\quad {disc}(r)=\frac{1}{\log_2(r+1)}$

---

## Step 5. 錯誤清單與改進建議
- **Missed-Known@K**：$g_{qi}\ge t,\ r_{qi}>K$（列名次、百分位、專家分數）。  
- **Low-Rated-in-K**：$r_{qi}\le K,\ g_{qi}\le 2$。  
- **Rank-Gap 分桶**：依差距分為輕 / 中 / 重，排定優先修正順序。  

利用這些清單分析特徵缺陷，調整系統排序權重或訓練 re-ranking 模型。  

---

## Step 6. 評估與迭代
- 整體上看 $\tau_b$、Pooled-NDCG、EWR 的平均與信賴區間。  
- 檢查錯誤清單是否縮小（Coverage 提升、漏排減少、平均 Gap 降低）。  
- 長期蒐集更多案例 → 迭代更新系統。
