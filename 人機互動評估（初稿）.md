Tier 2：人機互動評估（衡量品質）

這一層是衡量真正推薦品質的關鍵，也是上線前產生更可靠「金標」的方法。
分為兩部分，分別為請專家對新計劃評級以對已知審查名單題目做評級

方法：
對一組真實研究計畫產生 Top-N 推薦清單。
組成領域專家小組（理想為真實或前任召集人）。
向專家呈現計畫與匿名化的推薦名單（不顯示分數或排名）。
請專家依相關度評分（例如：1 = 無關、2 = 牽強、3 = 相關、4 = 高度相關）。
在已知委員資料中，請特別已知標記委員，即使在k名外
好處：
產生高品質標註資料：專家評分可成為更可靠的金標。
支援更合適的指標：有了分級的相關性分數後，可使用 NDCG@K（歸一化折損累積增益），能獎勵把高度相關項目排在前面，符合我們需求。
定性回饋：可收集專家對評分原因的質性意見，補足量化指標的不足。
注意：這不是純量化任務，人的判斷很重要。我會協助一起執行人機評估。
# 專家評級流程與演算法評估方案


## Step 1 專家標記
# 方案一
專家評級尺度為 **1=不重要、2=相關、3=重要、4=高度相關**。 （思考是否需要專家無法判斷時填‘空值‘）。  

我們的演算法在每題會先產生一個候選人排名，展示 **前 k=15 名（需討論，或許可隨機給）**並且打亂後供專家標記供專家挑選。

請專家事後想出心目中的人選（3 or 4）或不推薦委員（1），並說明理由


# 方案二

接續方案一再依照85分以上（需討論）分布，依照分數區間（視分布決定），隨機抽樣p人（需討論）。

此方案確保前面K人為適合人選，也確保後面人選排名極具合理性以及是否有漏網之魚。


# 疑問：
能請幾位專家，需要幾題題目，方案二隨機抽取名單是每位專家都一樣還是不一樣，如果不一樣後續方法更為複雜
，但對於結果推估更準確，是否在同評級但排名不一樣時請專家特別標記">"。
---

---
### Information Retrieval (IR) 指標
- **Precision@k**：前 k 名中相關評級候選比例。  
- **Recall@k**：前 k 名覆蓋了多少相關評級候選。
- ** RMSE **  
- **MAP (Mean Average Precision)**：多個正例時的平均精度。  
- **MRR (Mean Reciprocal Rank)**：首個正例的倒數排名平均。  
- **NDCG (Normalized Discounted Cumulative Gain)**  
- 適合有序等級（1–4），可用增益值映射 (如 0/1/3/7)。  
- 強調「重要的候選應該排在更前面」。  

### 統計方法（可思考有無必要）
- **置換檢定 (Permutation Test)**：檢查系統指標是否顯著優於隨機排序。  
## Step 2. 專家一致性檢定 (Inter-rater Reliability)

目的：確認不同專家對候選人的評級是否具有一致性，確保後續的共識標籤可信。  

常用指標：  

- **Krippendorff’s α（ordinal α）**  
  - 基於觀察分歧 (observed disagreement) 與期望分歧 (expected disagreement) 的比率。  
  - α = 1 完全一致；α = 0 等於隨機；α < 0 比隨機還差。  
  - 常用標準：α ≥ 0.67 視為可接受。  
  - **適用情境**：有序等級（例如 1–4）、能處理缺值。  

---

### 為什麼選這個指標？
	1.	最符合 ordinal 1–4 評級的資料特性（差距大小會被考慮）。
	2.	能處理缺值，比 κ 更實務。
	3.	學界常見標準，有較好說服力。

**公式核心：**

$$
\alpha = 1 - \frac{D_o}{D_e}
$$

- $D_o$：觀察到的分歧 (observed disagreement)  
- $D_e$：隨機情況下的期望分歧 (expected disagreement)  

在 **有序尺度 (ordinal scale)** 下，分歧的度量方式為：  

$$
\delta(c, c') = (c - c')^2
$$

也就是說，評級差距越大，懲罰越嚴重。  

---

**例子：**

假設有 2 位專家對 3 個候選人進行 1–4 分級：  

| 候選人 | 專家 A | 專家 B |
|--------|--------|--------|
| X      |   2    |   3    |
| Y      |   1    |   4    |
| Z      |   3    |   3    |

1. 計算每對標註的分歧：  
   - 候選人 X：$\delta(2,3) = (2-3)^2 = 1$
   - 候選人 Y：$\delta(1,4) = (1-4)^2 = 9$  
   - 候選人 Z：$\delta(3,3) = (3-3)^2 = 0$

   所以觀察到的平均分歧：  
   $$D_o = \frac{1+9+0}{3} = \frac{10}{3} \approx 3.33$$

2. 若評分是隨機的（期望分歧 $D_e$），假設計算得到 $D_e = 6$。  

3. 代入公式：  
   $$\alpha = 1 - \frac{D_o}{D_e} 
          = 1 - \frac{3.33}{6} 
          = 1 - 0.555 
          = 0.445$$

---

**解讀：**  
Krippendorff’s α = 0.445，代表專家之間有一定共識，但仍有分歧。  
若 α ≥ 0.67 通常被視為可接受的一致性。
---
## Step 3. 生成共識排名
若一致性足夠（無一致性跳到step5），可將專家評級整合成共識：  
- **平均分數法**：取所有專家分數平均，作為候選人分數。  
- **中位數 / 眾數**：避免極端值影響。  
- **加權平均**：依專家可靠度給權重。  

再依照分數高低轉換成 **共識排名**。

---

## Step 4. 系統排名 vs 專家共識比較
### Rank Correlation（秩相關）
- **Kendall’s τ-b**  
- 衡量兩個排名的一致性，能處理 ties。  
- 範圍 −1 ~ 1，值越高代表排序越一致。  
- **Somers’ D**  
- 指定方向的秩相關（例如「系統名次能否預測專家等級」）。  
- 與 AUC (Area Under Curve) 緊密關聯。  

### Trend Test（趨勢檢定）
- **Jonckheere–Terpstra test**  
- 檢定專家等級是否隨系統排名單調遞減。  



---

## Step 5. 若專家間不一致

當一致性指標（如 κ/α/W）偏低時，請先確認照成不一致原因
，接下來不建議直接產生單一共識標籤。可先做’逐專家比較‘。

---

### 5.1 逐專家比較（Per-expert evaluation + Wilcoxon 彙總）

**目的**  
在「沒有可靠共識」時，尊重每位專家的觀點，評估系統是否**整體傾向**與專家一致。

**做法**  
1. 對**每位**專家 `e`，計算「系統排名 vs 專家評級/排名」的秩相關（**Kendall’s τ-b** 或 **Somers’ D**）。  
2. 得到一組效應量：`{τ_e}` 或 `{D_e}`。  
3. 對 `{τ_e}`（或 `{D_e}`）做 **單尾 Wilcoxon 符號等級檢定**：  
   - H₀：中位數 = 0（整體無一致）  
   - H₁：中位數 > 0（整體呈正相關）  
4. 報告效應量（平均/中位數 ± CI）與 p 值；必要時再分層（題型、領域）。

**何時用**  
- 專家分歧大，**不宜硬做共識**。  
- 想回答「整體上，系統是否跟多數專家同向？」。

**優缺點**  
- ✅ 不強迫合一、統計上簡潔；能揭示「多數傾向」。  
- ⚠️ 仍需解釋離群專家（可做 leave-one-rater-out 檢視）。
### 5.2 仲裁 / 二輪標註（Adjudication / Second-round labeling）

**目的**  
透過溝通/校準，將「爭議樣本」聚焦處理，提升標註一致性與可用性。

**做法**  
1. **篩選爭議樣本**：如分散度高（分數方差/IQR 大）、評級分裂（如 1 與 4 並存）。  
2. **校準會議**：回顧標註指南與正反例，釐清定義與判準。  
3. **仲裁**：由資深專家或小組對爭議樣本給出最終標註（可保留備註理由）。  
4. **可選**：發佈「第二版指南」，再進行**二輪標註**以驗證改進（再算 κ/α）。

**何時用**  
- 需要**產出可落地的單一共識標籤**（供線上系統或訓練使用）。  
- 專家可投入少量時間開會或二次標註。

**優缺點**  
- ✅ 能矯正指南歧義，提升資料品質；實務可行。  
- ⚠️ 耗時；仍可能保留主觀性（建議記錄理由與決策依據）。
---
## NOTE : 專家共識不一致時的「逐專家比較」(step5) vs 專家一致時的「共識比較」(step2~4)
### 核心差異（TL;DR）
- **共識比較（專家一致時）**：把多位專家的評級合成**一套「地面實況」(ground truth)**，再檢定**系統 vs 這套地面實況**是否一致。
- **逐專家比較（專家不一致時）**：保留**每位專家的個別觀點**，先算**系統 vs 各專家**的相關係數，再用 **Wilcoxon** 檢定「**多數專家**是否與系統**整體同向**」。

### 對照表

| 面向 | 共識比較（一致性高時用） | 逐專家比較（一致性低時用） |
|---|---|---|
| **輸入標籤** | 已**聚合**成單一共識（平均/中位/加權/Borda…） | **未聚合**，保留每位專家的原始評級/排序 |
| **分析單位** | 以**題目**為單位算指標（NDCG@k、τ-b、Somers’ D），再彙總/CI | 以**專家**為單位得到一組相關值 {τ\_e 或 D\_e} |
| **虛無假設 H₀** | 系統與**共識標籤**無關（例如平均 NDCG 無法區分、或 τ-b=0） | 這組 {τ\_e}（或 {D\_e}）的**中位數 = 0**（整體無正向趨勢） |
| **檢定方法** | 置換檢定（對 NDCG/τ-b）、或對題目層差值做 Wilcoxon/Bootstrap CI | **Wilcoxon 符號等級**（單尾，檢中位數>0） |
| **回答的問題** | 「**系統是否和**『一套被認可的**共識真值**』**一致**？」 | 「**在多數專家眼裡**，系統是否**傾向**與他們一致？」（即使少數專家不同意） |
| **什麼時候用** | κ/α/W 顯示**一致性高**（例如 α≥0.67） | κ/α/W 顯示**一致性低/中**，不宜硬做共識 |
| **風險/侷限** | 若其實有兩個專家派系，硬聚合會**稀釋/扭曲**立場 | 少數離群專家會被「多數決」**壓過**，但保留了異質性 |

### 為何聽起來像「都在檢一致」？
- 名義上都在問「系統與專家是否一致」，但**主詞不同**：  
  - **共識比較**：系統 vs **單一共識**（把專家們視為「一個聲音」）。  
  - **逐專家比較**：系統 vs **每位專家**（承認有多個聲音），最後問「**多數**是否支持系統」（Wilcoxon）。

### 兩者可能得出不同結論的例子（直觀）
- 有 3 位專家：A、B 與系統高度一致（τ≈0.6），C 幾乎反向（τ≈-0.2）。  
- **逐專家比較**：{0.6, 0.55, -0.2} → Wilcoxon 仍可能顯著「中位數>0」，結論：**多數專家支持系統**。  
- **若硬做共識**：A/B 與 C 的觀點互相抵銷，合成標籤變得「溫和/模糊」，系統 vs 共識的 τ 可能只有 ~0.2 → **不顯著**。  
→ 說明兩條路線**回答的是不同問題**：一條問「對單一真值的符合度」，一條問「是否得到多數專家的支持」。

### 實務決策建議
1. **先算一致性（κ/α/W）**：  
   - **高** → 走「共識比較」。  
   - **低/中** → 走「逐專家比較（+ Wilcoxon）」；必要時做仲裁/二輪或 DS/MACE。  
2. **透明度**：即便走共識，也可附帶「逐專家結果分佈」當附錄，避免共識掩蓋爭議。
---



---

## Step 6. 解讀
- 若所有前 15 候選人均被評為「相關 (≥2)」 → 系統能有效產生高品質候選集。  
- 此時 Precision@15 幾乎為 100%，重點轉移到：  
- 系統是否能將「高度相關 (4)」者排在更前面？  
- 評估宜使用 **NDCG@k、Somers’ D、Kendall’s τ-b** 等排序敏感指標。
- 如果皆系統與專家皆不一致，要知道專家排名理由、演算法核心是否需要修改，找出不一致理由。
🔑 可以再做的事
---





---

## 已知委員特別統計

維持與 Part A 相同的步驟（專家評級 → 一致性檢定 → 系統 vs 專家比較）。  
差別在於：**額外切出「已知委員」子集合，做專門統計與診斷。**

### 位置面
- 已知委員覆蓋率（Coverage@K）  
  $\mathrm{Coverage@}K_{\text{known}}(q)=\frac{|G_q\cap \mathrm{TopK}_q|}{|G_q|}$  

- 平均/中位名次：已知委員在整體排序中的平均或中位數排名。  

- K 外已知委員的平均名次（量化「漏掉多遠」）。  

- 百分位：  
  ${Pct}_{qi}=1-\frac{r_{qi}-1}{N_q}$  

### 品質面
- HighRel-in-K：已知委員進到 Top-K 中，被專家評為「重要以上」的比例。  

- Missed-Known@K：已知委員在 K 外，但專家評高分 → 系統漏排案例。  

- Rank-Gap：  
  ${Gap}_{qi}=r_{qi}-K$  
  用來區分「輕微漏排（16 名）」與「嚴重漏排（30 名）」。

- 曝光加權召回（EWR）：考慮名次折扣，衡量高分已知委員被排到前面的程度。  

### 簡易診斷表
對已知委員做 2×2 分類：  
- TP：進 K 且高分  
- FP：進 K 但低分  
- FN：在 K 外但高分（漏排）  
- TN：在 K 外且低分  

---

**好處**  
- 保持主流程簡單。  
- 已知委員額外統計，能清楚回答「系統是否漏掉重要人選？漏得多遠？排進來的品質好不好？」  
- 結果可直接用來改進排序演算法（調整特徵或 re-ranking）。

## 專家標記前，推薦委員
請先專家標記前先推薦委員，並說明理由，
統計是否優多位專家共同推薦，是在K外還是K內，在我們系統推薦名單又是排名多少


#想法
如果分領域表現一致也不錯是否可以用ListMLE...等機器學習

