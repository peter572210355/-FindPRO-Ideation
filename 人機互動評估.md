Tier 2：人機互動評估（衡量品質）

這一層是衡量真正推薦品質的關鍵，也是上線前產生更可靠「金標」的方法。
分為兩部分，分別為請專家對新計劃評級以對已知審查名單題目做評級

方法：
對一組真實研究計畫產生 Top-N 推薦清單。
組成領域專家小組（理想為真實或前任召集人）。
向專家呈現計畫與匿名化的推薦名單（不顯示分數或排名）。
請專家依相關度評分（例如：1 = 無關、2 = 牽強、3 = 相關、4 = 高度相關）。
好處：
產生高品質標註資料：專家評分可成為更可靠的金標。
支援更合適的指標：有了分級的相關性分數後，可使用 NDCG@K（歸一化折損累積增益），能獎勵把高度相關項目排在前面，符合我們需求。
定性回饋：可收集專家對評分原因的質性意見，補足量化指標的不足。
注意：這不是純量化任務，人的判斷很重要。我會協助一起執行人機評估。
# 專家評級流程與演算法評估方案

此文件說明如何利用多位專家對新題目的評級結果，來檢驗並更新我們的演算法排名。  
專家評級尺度為 **1=不重要、2=相關、3=重要、4=高度相關**。 （思考是否需要專家無法判斷值為空值） 
我們的演算法在每題會先產生一個候選人排名，展示 **前 k=15 名**供專家挑選。

---

---

## Step 2. 專家一致性檢定 (Inter-rater Reliability)

目的：確認不同專家對候選人的評級是否具有一致性，確保後續的共識標籤可信。  

常用指標：  

- **Krippendorff’s α（ordinal α）**  
  - 基於觀察分歧 (observed disagreement) 與期望分歧 (expected disagreement) 的比率。  
  - α = 1 完全一致；α = 0 等於隨機；α < 0 比隨機還差。  
  - 常用標準：α ≥ 0.67 視為可接受。  
  - **適用情境**：有序等級（例如 1–4）、能處理缺值。  

---

### 為什麼選這個指標？
	1.	最符合 ordinal 1–4 評級的資料特性（差距大小會被考慮）。
	2.	能處理缺值，比 κ 更實務。
	3.	學界常見標準，有較好說服力。
 ### Kendall’s W（秩和一致性係數）

### Krippendorff’s α（Ordinal α）

**公式核心：**

$$
\alpha = 1 - \frac{D_o}{D_e}
$$

- $D_o$：觀察到的分歧 (observed disagreement)  
- $D_e$：隨機情況下的期望分歧 (expected disagreement)  

在 **有序尺度 (ordinal scale)** 下，分歧的度量方式為：  

$$
\delta(c, c') = (c - c')^2
$$

也就是說，評級差距越大，懲罰越嚴重。  

---

**例子：**

假設有 2 位專家對 3 個候選人進行 1–4 分級：  

| 候選人 | 專家 A | 專家 B |
|--------|--------|--------|
| X      |   2    |   3    |
| Y      |   1    |   4    |
| Z      |   3    |   3    |

1. 計算每對標註的分歧：  
   - 候選人 X：$\delta(2,3) = (2-3)^2 = 1$
   - 候選人 Y：$\delta(1,4) = (1-4)^2 = 9$  
   - 候選人 Z：$\delta(3,3) = (3-3)^2 = 0$

   所以觀察到的平均分歧：  
   $$D_o = \frac{1+9+0}{3} = \frac{10}{3} \approx 3.33$$

2. 若評分是隨機的（期望分歧 $D_e$），假設計算得到 $D_e = 6$。  

3. 代入公式：  
   $$\alpha = 1 - \frac{D_o}{D_e} 
          = 1 - \frac{3.33}{6} 
          = 1 - 0.555 
          = 0.445$$

---

**解讀：**  
Krippendorff’s α = 0.445，代表專家之間有一定共識，但仍有分歧。  
若 α ≥ 0.67 通常被視為可接受的一致性。
---
## Step 3. 生成共識標籤 (Consensus Labeling)
若一致性足夠，可將專家評級整合成共識：  
- **平均分數法**：取所有專家分數平均，作為候選人分數。  
- **中位數 / 眾數**：避免極端值影響。  
- **加權平均**：依專家可靠度給權重。  

再依照分數高低轉換成 **共識排名**。

---

## Step 4. 系統排名 vs 專家共識比較
### Rank Correlation（秩相關）
- **Kendall’s τ-b**  
- 衡量兩個排名的一致性，能處理 ties。  
- 範圍 −1 ~ 1，值越高代表排序越一致。  
- **Somers’ D**  
- 指定方向的秩相關（例如「系統名次能否預測專家等級」）。  
- 與 AUC (Area Under Curve) 緊密關聯。  

### Trend Test（趨勢檢定）
- **Jonckheere–Terpstra test**  
- 檢定專家等級是否隨系統排名單調遞減。  

### Information Retrieval (IR) 指標
- **Precision@k**：前 k 名中相關候選比例。  
- **Recall@k**：前 k 名覆蓋了多少已知相關候選。  
- **HitRate@k**：前 k 名中是否至少有 1 個相關候選。  
- **MAP (Mean Average Precision)**：多個正例時的平均精度。  
- **MRR (Mean Reciprocal Rank)**：首個正例的倒數排名平均。  
- **NDCG (Normalized Discounted Cumulative Gain)**  
- 適合有序等級（1–4），可用增益值映射 (如 0/1/3/7)。  
- 強調「重要的候選應該排在更前面」。  

### 統計檢定
- **Wilcoxon 符號等級檢定**：檢定系統 vs 專家相關性是否顯著大於 0。  
- **置換檢定 (Permutation Test)**：檢查系統指標是否顯著優於隨機排序。  
- **Bootstrap 信賴區間**：以題目為單位抽樣，估計指標區間。  

---

## Step 5. 若專家間不一致
- **逐專家比較**：保留每位專家的評級，分別與系統比較，最後用 Wilcoxon 彙總。  
- **仲裁/二輪標註**：召集專家會議，針對爭議案例重新標註。  
- **機率模型**：用 Dawid–Skene / MACE 推估「潛在真實標籤」。  

---

## Step 6. 解讀
- 若所有前 15 候選人均被評為「相關 (≥2)」 → 系統能有效產生高品質候選集。  
- 此時 Precision@15 幾乎為 100%，重點轉移到：  
- 系統是否能將「高度相關 (4)」者排在更前面？  
- 評估宜使用 **NDCG@k、Somers’ D、Kendall’s τ-b** 等排序敏感指標。  

🔑 可以再做的事

1. 偏好式標記 (Pairwise / Preference Feedback)
	•	不一定要專家為每個候選人打分數（1–4 分），也可以用比較的方式：
「對這個題目，A 比 B 更合適嗎？」
	•	好處：專家更容易回答，負擔小；資料可以用來訓練 Learning-to-Rank 模型。

⸻

2. 專家一致性 / 可信度分析
	•	同一題給多位專家標記，檢查 一致性 (inter-rater agreement)：
	•	Cohen’s kappa、Krippendorff’s alpha
	•	若一致性低，代表題目本身有模糊性，系統也不容易排準。
	•	可以把一致性當作「題目難度」的 proxy。

⸻

3. 錯誤案例訪談 (Qualitative Study)
	•	挑出模型與專家分歧最大的題目，請專家說明原因：
	•	系統忽略了什麼特徵？（例如：跨領域背景）
	•	題目描述有歧義？
	•	這些質性意見可以直接指導下一步特徵工程或模型改進。

⸻

4. 分領域檢查
	•	不同領域可能標記量足夠 / 不足。
	•	可以檢查：
	•	哪些領域容易達成高 NDCG？
	•	哪些領域即使專家標了，系統仍然差距大？
	•	結果能幫助你判斷「需要針對哪些領域做特化模型」。

⸻

5. 多輪互動 (Iterative Labeling)
	•	不是一次把所有推薦名單丟給專家，而是 逐輪互動：
	•	系統先出 Top-10 → 專家標記 → 模型更新
	•	再產生新 Top-10 → 專家再標
	•	每輪都能檢查改進幅度，像 active learning 一樣收斂。

⸻

6. 時間成本 / 可用性分析
	•	除了看準確度，也可以記錄專家 標記一份題目所需的時間。
	•	若系統排序越好，專家標記時間應該會更短。
	•	這是一個「人因工程」角度的評估：不只是準確，也要節省專家工作量。

