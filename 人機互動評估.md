Tier 2：人機互動評估（衡量品質）

這一層是衡量真正推薦品質的關鍵，也是上線前產生更可靠「金標」的方法。
分為兩部分，分別為請專家對新計劃評級以對已知審查名單題目做評級

方法：
對一組真實研究計畫產生 Top-N 推薦清單。
組成領域專家小組（理想為真實或前任召集人）。
向專家呈現計畫與匿名化的推薦名單（不顯示分數或排名）。
請專家依相關度評分（例如：1 = 無關、2 = 牽強、3 = 相關、4 = 高度相關）。
好處：
產生高品質標註資料：專家評分可成為更可靠的金標。
支援更合適的指標：有了分級的相關性分數後，可使用 NDCG@K（歸一化折損累積增益），能獎勵把高度相關項目排在前面，符合我們需求。
定性回饋：可收集專家對評分原因的質性意見，補足量化指標的不足。
注意：這不是純量化任務，人的判斷很重要。我會協助一起執行人機評估。
# 專家評級流程與演算法評估方案

此文件說明如何利用多位專家對新題目的評級結果，來檢驗並更新我們的演算法排名。  
專家評級尺度為 **1=不重要、2=相關、3=重要、4=高度相關**。 （思考是否需要專家無法判斷值為空值） 
我們的演算法在每題會先產生一個候選人排名，展示 **前 k=15 名**供專家挑選。

---

---

## Step 2. 專家一致性檢定 (Inter-rater Reliability)

目的：確認不同專家對候選人的評級是否具有一致性，確保後續的共識標籤可信。  

常用指標：  

- **Krippendorff’s α（ordinal α）**  
  - 基於觀察分歧 (observed disagreement) 與期望分歧 (expected disagreement) 的比率。  
  - α = 1 完全一致；α = 0 等於隨機；α < 0 比隨機還差。  
  - 常用標準：α ≥ 0.67 視為可接受。  
  - **適用情境**：有序等級（例如 1–4）、能處理缺值。  

---

### 為什麼選這個指標？
	1.	最符合 ordinal 1–4 評級的資料特性（差距大小會被考慮）。
	2.	能處理缺值，比 κ 更實務。
	3.	學界常見標準，有較好說服力。

**公式核心：**

$$
\alpha = 1 - \frac{D_o}{D_e}
$$

- $D_o$：觀察到的分歧 (observed disagreement)  
- $D_e$：隨機情況下的期望分歧 (expected disagreement)  

在 **有序尺度 (ordinal scale)** 下，分歧的度量方式為：  

$$
\delta(c, c') = (c - c')^2
$$

也就是說，評級差距越大，懲罰越嚴重。  

---

**例子：**

假設有 2 位專家對 3 個候選人進行 1–4 分級：  

| 候選人 | 專家 A | 專家 B |
|--------|--------|--------|
| X      |   2    |   3    |
| Y      |   1    |   4    |
| Z      |   3    |   3    |

1. 計算每對標註的分歧：  
   - 候選人 X：$\delta(2,3) = (2-3)^2 = 1$
   - 候選人 Y：$\delta(1,4) = (1-4)^2 = 9$  
   - 候選人 Z：$\delta(3,3) = (3-3)^2 = 0$

   所以觀察到的平均分歧：  
   $$D_o = \frac{1+9+0}{3} = \frac{10}{3} \approx 3.33$$

2. 若評分是隨機的（期望分歧 $D_e$），假設計算得到 $D_e = 6$。  

3. 代入公式：  
   $$\alpha = 1 - \frac{D_o}{D_e} 
          = 1 - \frac{3.33}{6} 
          = 1 - 0.555 
          = 0.445$$

---

**解讀：**  
Krippendorff’s α = 0.445，代表專家之間有一定共識，但仍有分歧。  
若 α ≥ 0.67 通常被視為可接受的一致性。
---
## Step 3. 生成共識標籤 (Consensus Labeling)
若一致性足夠（無一致性跳到step5），可將專家評級整合成共識：  
- **平均分數法**：取所有專家分數平均，作為候選人分數。  
- **中位數 / 眾數**：避免極端值影響。  
- **加權平均**：依專家可靠度給權重。  

再依照分數高低轉換成 **共識排名**。

---

## Step 4. 系統排名 vs 專家共識比較
### Rank Correlation（秩相關）
- **Kendall’s τ-b**  
- 衡量兩個排名的一致性，能處理 ties。  
- 範圍 −1 ~ 1，值越高代表排序越一致。  
- **Somers’ D**  
- 指定方向的秩相關（例如「系統名次能否預測專家等級」）。  
- 與 AUC (Area Under Curve) 緊密關聯。  

### Trend Test（趨勢檢定）
- **Jonckheere–Terpstra test**  
- 檢定專家等級是否隨系統排名單調遞減。  

### Information Retrieval (IR) 指標
- **Precision@k**：前 k 名中相關候選比例。  
- **Recall@k**：前 k 名覆蓋了多少已知相關候選。    
- **MAP (Mean Average Precision)**：多個正例時的平均精度。  
- **MRR (Mean Reciprocal Rank)**：首個正例的倒數排名平均。  
- **NDCG (Normalized Discounted Cumulative Gain)**  
- 適合有序等級（1–4），可用增益值映射 (如 0/1/3/7)。  
- 強調「重要的候選應該排在更前面」。  

### 統計方法（可思考有無必要）
- **置換檢定 (Permutation Test)**：檢查系統指標是否顯著優於隨機排序。  

---

## Step 5. 若專家間不一致

當一致性指標（如 κ/α/W）偏低時，不建議直接產生單一共識標籤。以下三種路線可擇一或搭配使用。

---

### 5.1 逐專家比較（Per-expert evaluation + Wilcoxon 彙總）

**目的**  
在「沒有可靠共識」時，尊重每位專家的觀點，評估系統是否**整體傾向**與專家一致。

**做法**  
1. 對**每位**專家 `e`，計算「系統排名 vs 專家評級/排名」的秩相關（**Kendall’s τ-b** 或 **Somers’ D**）。  
2. 得到一組效應量：`{τ_e}` 或 `{D_e}`。  
3. 對 `{τ_e}`（或 `{D_e}`）做 **單尾 Wilcoxon 符號等級檢定**：  
   - H₀：中位數 = 0（整體無一致）  
   - H₁：中位數 > 0（整體呈正相關）  
4. 報告效應量（平均/中位數 ± CI）與 p 值；必要時再分層（題型、領域）。

**何時用**  
- 專家分歧大，**不宜硬做共識**。  
- 想回答「整體上，系統是否跟多數專家同向？」。

**優缺點**  
- ✅ 不強迫合一、統計上簡潔；能揭示「多數傾向」。  
- ⚠️ 仍需解釋離群專家（可做 leave-one-rater-out 檢視）。
### 5.2 仲裁 / 二輪標註（Adjudication / Second-round labeling）

**目的**  
透過溝通/校準，將「爭議樣本」聚焦處理，提升標註一致性與可用性。

**做法**  
1. **篩選爭議樣本**：如分散度高（分數方差/IQR 大）、評級分裂（如 1 與 4 並存）。  
2. **校準會議**：回顧標註指南與正反例，釐清定義與判準。  
3. **仲裁**：由資深專家或小組對爭議樣本給出最終標註（可保留備註理由）。  
4. **可選**：發佈「第二版指南」，再進行**二輪標註**以驗證改進（再算 κ/α）。

**何時用**  
- 需要**產出可落地的單一共識標籤**（供線上系統或訓練使用）。  
- 專家可投入少量時間開會或二次標註。

**優缺點**  
- ✅ 能矯正指南歧義，提升資料品質；實務可行。  
- ⚠️ 耗時；仍可能保留主觀性（建議記錄理由與決策依據）。
---
## NOTE : 專家不一致時的「逐專家比較」(step5) vs 專家一致時的「共識比較」(step2~4)
### 核心差異（TL;DR）
- **共識比較（專家一致時）**：把多位專家的評級合成**一套「地面實況」(ground truth)**，再檢定**系統 vs 這套地面實況**是否一致。
- **逐專家比較（專家不一致時）**：保留**每位專家的個別觀點**，先算**系統 vs 各專家**的相關係數，再用 **Wilcoxon** 檢定「**多數專家**是否與系統**整體同向**」。

### 對照表

| 面向 | 共識比較（一致性高時用） | 逐專家比較（一致性低時用） |
|---|---|---|
| **輸入標籤** | 已**聚合**成單一共識（平均/中位/加權/Borda…） | **未聚合**，保留每位專家的原始評級/排序 |
| **分析單位** | 以**題目**為單位算指標（NDCG@k、τ-b、Somers’ D），再彙總/CI | 以**專家**為單位得到一組相關值 {τ\_e 或 D\_e} |
| **虛無假設 H₀** | 系統與**共識標籤**無關（例如平均 NDCG 無法區分、或 τ-b=0） | 這組 {τ\_e}（或 {D\_e}）的**中位數 = 0**（整體無正向趨勢） |
| **檢定方法** | 置換檢定（對 NDCG/τ-b）、或對題目層差值做 Wilcoxon/Bootstrap CI | **Wilcoxon 符號等級**（單尾，檢中位數>0） |
| **回答的問題** | 「**系統是否和**『一套被認可的**共識真值**』**一致**？」 | 「**在多數專家眼裡**，系統是否**傾向**與他們一致？」（即使少數專家不同意） |
| **什麼時候用** | κ/α/W 顯示**一致性高**（例如 α≥0.67） | κ/α/W 顯示**一致性低/中**，不宜硬做共識 |
| **風險/侷限** | 若其實有兩個專家派系，硬聚合會**稀釋/扭曲**立場 | 少數離群專家會被「多數決」**壓過**，但保留了異質性 |

### 為何聽起來像「都在檢一致」？
- 名義上都在問「系統與專家是否一致」，但**主詞不同**：  
  - **共識比較**：系統 vs **單一共識**（把專家們視為「一個聲音」）。  
  - **逐專家比較**：系統 vs **每位專家**（承認有多個聲音），最後問「**多數**是否支持系統」（Wilcoxon）。

### 兩者可能得出不同結論的例子（直觀）
- 有 3 位專家：A、B 與系統高度一致（τ≈0.6），C 幾乎反向（τ≈-0.2）。  
- **逐專家比較**：{0.6, 0.55, -0.2} → Wilcoxon 仍可能顯著「中位數>0」，結論：**多數專家支持系統**。  
- **若硬做共識**：A/B 與 C 的觀點互相抵銷，合成標籤變得「溫和/模糊」，系統 vs 共識的 τ 可能只有 ~0.2 → **不顯著**。  
→ 說明兩條路線**回答的是不同問題**：一條問「對單一真值的符合度」，一條問「是否得到多數專家的支持」。

### 實務決策建議
1. **先算一致性（κ/α/W）**：  
   - **高** → 走「共識比較」。  
   - **低/中** → 走「逐專家比較（+ Wilcoxon）」；必要時做仲裁/二輪或 DS/MACE。  
2. **透明度**：即便走共識，也可附帶「逐專家結果分佈」當附錄，避免共識掩蓋爭議。
---



---

## Step 6. 解讀
- 若所有前 15 候選人均被評為「相關 (≥2)」 → 系統能有效產生高品質候選集。  
- 此時 Precision@15 幾乎為 100%，重點轉移到：  
- 系統是否能將「高度相關 (4)」者排在更前面？  
- 評估宜使用 **NDCG@k、Somers’ D、Kendall’s τ-b** 等排序敏感指標。
- 如果皆系統與專家皆不一致，要知道專家排名理由、演算法核心是否需要修改，找出不一致理由。
🔑 可以再做的事
---
# Part B：已知審查委員的排名狀況（只展示 Top-k）

本節說明如何在僅展示 **Top-k (=15)** 供專家評級的前提下，系統性描述「**已知委員**」在完整排序中的位置與品質。

---

## 資料定義

- 對每題 $q$，系統給所有候選委員 $i$ 一個完整排名 $(r_{qi}$（1=最前），總數 $N_q4。  
- 只展示 $TopK_q=\{i:\, r_{qi}\le k\}$ 供專家評級，得到 $g_{qi}\in\{1,2,3,4\}$（1=不重要，4=高度相關）。  
- 已知委員集合 $G_q$（針對該題你關注/已知的委員；可能相關也可能不相關）。  
- 未展示者（$r_{qi}>k$）沒有評級（標成 `NA`）。

---

## 流程（不改動只看 Top-k 的前提）

1. **產出「已知委員狀態表」**（每題一張）  
   欄位：`reviewer_id, r, in_topk, expert_grade(g或NA), pct_rank, exposure`  
   - 百分位：  
     $\text{pct\_rank}_{qi}=1-\frac{r_{qi}-1}{N_q}$
   - 曝光折扣：  
     $\text{exposure}_{qi}=\frac{1}{\log_2(r_{qi}+1)}$

2. **將已知委員分桶**（四格看板，易讀）  
   - **A**：在 k 內且 $g\ge 3$（命中且高質）  
   - **B**：在 k 內且 $g\le 2$（命中但品質普通/不符）  
   - **C**：在 k 外（**未評**，只報位置）  
   - **D**：在 k 內但未評（若偶有缺值）

3. **報指標（位置面 + 質量面，分開看）**  
   - 位置面：不需評級，能覆蓋 **k 內/外**  
   - 質量面：只基於 **k 內** 的專家評級

---

## 指標設計

### 一、位置面（不依賴評級，能描述 k 內/外）

- **Coverage@K（覆蓋率）**  
  $\mathrm{Coverage@}K(q)=\frac{|G_q\cap \mathrm{TopK}_q|}{|G_q|}$

- **Success@K（至少命中一位）**  
  $\mathrm{Success@}K(q)=\mathbf{1}\!\left(|G_q\cap \mathrm{TopK}_q|>0\right)$

- **Rank 分佈統計（對 $G_q$）**  
  - 中位名次：$\mathrm{median}\{r_{qi}:i\in G_q\}$  
  - 90 分位名次：$\mathrm{p90}\{r_{qi}:i\in G_q\}$  
  - 後段比例：  
    $\frac{|\{i\in G_q: r_{qi}>0.8N_q\}|}{|G_q|}$

- **Exposure Share（曝光占比，對 $G_q$）**  
  $\mathrm{ExpoShare}(q)=\frac{\sum_{i\in G_q}\text{exposure}_{qi}}{\sum_{j}\text{exposure}_{qj}}$

- **Rank-CDF 曲線（診斷圖）**  
  $F_q(r)=\Pr(r_{qi}\le r \mid i\in G_q)$

---

### 二、質量面（只基於 k 內的專家評級）

- **Known-in-K 的高質占比**  
  $\mathrm{HighRel\ in\ K}(q)=\frac{|\{i\in G_q\cap \mathrm{TopK}_q:\ g_{qi}\ge 3\}|}{|G_q\cap \mathrm{TopK}_q|\vee 1}$

- **NDCG@K（分級增益，僅限 Top-k）**  
  - 對 $\mathrm{TopK}_q$ 以 $g$ 轉增益（例：$0,1,3,7$）計 DCG  
  - 除以 iDCG 得到 NDCG  
  - 平均到題目層  
    $\mathrm{NDCG@}K(q)=\frac{\mathrm{DCG@}K(q)}{\mathrm{iDCG@}K(q)}$

- **誤差清單（優先修正用）**  
  - **Missed-Known@K**：$i\in G_q, r_{qi}>k$（未評，列位置）  
  - **Low-Rated-Known@K**：$i\in G_q\cap \mathrm{TopK}_q, g_{qi}\le 2$

---

## 報表建議

- **每題摘要卡**  
  - Coverage@K、Success@K  
  - $G_q$ 的中位/90 分位名次、ExpoShare  
  - k 內已知委員的 $g$ 分布（柱狀）  
  - Missed-Known@K 名單（列出 $r$、百分位）

- **整體儀表板**（跨題彙總）  
  - 平均 Coverage@K、Success@K  
  - Rank-CDF 疊圖（多題平均）  
  - NDCG@K（僅 Top-k）＋ 95% Bootstrap CI（以題為單位抽樣）

---

## 限制與說明

- **僅展示 Top-k → 標註截斷**  
  - k 外者沒有評級，因此**不能**對其做品質判斷；本設計**明確區分「位置診斷」與「品質診斷」**，避免過度推論。  

- **已知委員不一定都是正例**  
  - 本設計**不預設**，而是由 $g$ 決定質量：  
    - 在 k 內：有 $g$ 可評「質量」  
    - 在 k 外：無 $g$，只報「位置」  

---







