Tier 2：人機互動評估（衡量品質）

這一層是衡量真正推薦品質的關鍵，也是上線前產生更可靠「金標」的方法。

方法：
對一組真實研究計畫產生 Top-N 推薦清單。
組成領域專家小組（理想為真實或前任召集人）。
向專家呈現計畫與匿名化的推薦名單（不顯示分數或排名）。
請專家依相關度評分（例如：1 = 無關、2 = 牽強、3 = 相關、4 = 高度相關）。
好處：
產生高品質標註資料：專家評分可成為更可靠的金標。
支援更合適的指標：有了分級的相關性分數後，可使用 NDCG@K（歸一化折損累積增益），能獎勵把高度相關項目排在前面，符合我們需求。
定性回饋：可收集專家對評分原因的質性意見，補足量化指標的不足。
注意：這不是純量化任務，人的判斷很重要。我會協助一起執行人機評估。



🔑 可以再做的事

1. 偏好式標記 (Pairwise / Preference Feedback)
	•	不一定要專家為每個候選人打分數（1–4 分），也可以用比較的方式：
「對這個題目，A 比 B 更合適嗎？」
	•	好處：專家更容易回答，負擔小；資料可以用來訓練 Learning-to-Rank 模型。

⸻

2. 專家一致性 / 可信度分析
	•	同一題給多位專家標記，檢查 一致性 (inter-rater agreement)：
	•	Cohen’s kappa、Krippendorff’s alpha
	•	若一致性低，代表題目本身有模糊性，系統也不容易排準。
	•	可以把一致性當作「題目難度」的 proxy。

⸻

3. 錯誤案例訪談 (Qualitative Study)
	•	挑出模型與專家分歧最大的題目，請專家說明原因：
	•	系統忽略了什麼特徵？（例如：跨領域背景）
	•	題目描述有歧義？
	•	這些質性意見可以直接指導下一步特徵工程或模型改進。

⸻

4. 分領域檢查
	•	不同領域可能標記量足夠 / 不足。
	•	可以檢查：
	•	哪些領域容易達成高 NDCG？
	•	哪些領域即使專家標了，系統仍然差距大？
	•	結果能幫助你判斷「需要針對哪些領域做特化模型」。

⸻

5. 多輪互動 (Iterative Labeling)
	•	不是一次把所有推薦名單丟給專家，而是 逐輪互動：
	•	系統先出 Top-10 → 專家標記 → 模型更新
	•	再產生新 Top-10 → 專家再標
	•	每輪都能檢查改進幅度，像 active learning 一樣收斂。

⸻

6. 時間成本 / 可用性分析
	•	除了看準確度，也可以記錄專家 標記一份題目所需的時間。
	•	若系統排序越好，專家標記時間應該會更短。
	•	這是一個「人因工程」角度的評估：不只是準確，也要節省專家工作量。

