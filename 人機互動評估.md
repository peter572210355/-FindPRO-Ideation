Tier 2：人機互動評估（衡量品質）

這一層是衡量真正推薦品質的關鍵，也是上線前產生更可靠「金標」的方法。
分為兩部分，分別為請專家對新計劃評級以對已知審查名單題目做評級

方法：
對一組真實研究計畫產生 Top-N 推薦清單。
組成領域專家小組（理想為真實或前任召集人）。
向專家呈現計畫與匿名化的推薦名單（不顯示分數或排名）。
請專家依相關度評分（例如：1 = 無關、2 = 牽強、3 = 相關、4 = 高度相關）。
好處：
產生高品質標註資料：專家評分可成為更可靠的金標。
支援更合適的指標：有了分級的相關性分數後，可使用 NDCG@K（歸一化折損累積增益），能獎勵把高度相關項目排在前面，符合我們需求。
定性回饋：可收集專家對評分原因的質性意見，補足量化指標的不足。
注意：這不是純量化任務，人的判斷很重要。我會協助一起執行人機評估。
# 專家評級流程與演算法評估方案

此文件說明如何利用多位專家對新題目的評級結果，來檢驗並更新我們的演算法排名。  
專家評級尺度為 **1=不重要、2=相關、3=重要、4=高度相關**。  
我們的演算法在每題會先產生一個候選人排名，展示 **前 k=15 名**供專家挑選。

---

## Step 1. 專家評級收集
1. 每題輸入題目，演算法產生候選人名單。
2. 多位專家（例如 3–5 人）對候選人進行 **四級評級 (1–4)**。
3. 收集成表格資料：`query_id, candidate_id, system_rank, expert_id, expert_grade`。

---

## Step 2. 確認專家一致性
- **目的**：評估不同專家之間對候選人重要性的共識程度。  
- **方法**：
  - 有序類別 → **加權 Fleiss’ κ** 或 **Krippendorff’s α (ordinal)**。
  - 若專家直接提供完整名次 → **Kendall’s W**。
- **解讀**：
  - κ/α ≥ 0.60 → 一致性可接受，可聚合成共識。
  - 0.30–0.59 → 存疑，需進一步校準或仲裁。
  - <0.30 → 不一致，需重新檢討標準或增加專家。

---

## Step 3. 生成專家共識
若一致性達標，將專家評級整合成共識：
- **平均分數法**：取所有專家評級平均，得到每個候選人的共識分數。  
- **中位數 / 眾數**：避免極端值影響。  
- **加權平均**：根據專家可靠度 (例如 leave-one-out κ 值) 給予權重。  

再依照共識分數高低 → 轉換成 **專家共識排名**。

---

## Step 4. 系統排名 vs 專家共識比較
可使用多種方法比較演算法排名與專家共識排名的一致性：

### 相關係數方法
- **Kendall’s τ-b**：處理 ties，對稱相關。
- **Somers’ D**：指定方向（系統排序是否能預測專家等級），與 AUC 緊密關聯。

### 趨勢檢定
- **Jonckheere–Terpstra test**：檢定是否存在單調趨勢（評級越高 → 系統名次越前）。

### IR 任務指標
- **Precision@k、Recall@k、HitRate@k**  
- **MAP / MRR**  
- **NDCG@k**（需先將專家評級映成增益值，如 0/1/3/7）

### 檢定方式
- **無母數檢定**：Wilcoxon 符號等級檢定（逐題效應量中位數是否 >0）、置換檢定（隨機打亂標籤檢驗 NDCG 是否顯著優於隨機）。  
- **統計檢定**：以題目為單位計算差異，再做成對 t 檢定或 Wilcoxon。

---

## Step 5. 若專家評分不一致
當專家一致性不足時，不宜直接做共識排名。可選方案：
1. **逐專家比較**：對每位專家分別計算 τ-b / Somers’ D，最後用 **Wilcoxon 符號等級檢定**彙總。  
2. **仲裁/二輪標註**：針對爭議樣本召集專家討論，或請第三方資深專家仲裁。  
3. **機率模型**：用 Dawid–Skene、MACE 或 IRT 等方法推估候選人的「潛在真實等級」。  
4. **回報分歧**：若共識難以達成，報告時強調專家間存在差異，不強行產出單一排名。

---

## Step 6. 持續更新演算法
- 當專家判斷「重要候選人」卻被系統排到很後面：
  - **誤差分析**：找出落差案例，理解專家 vs 系統差異原因。  
  - **成對學習微調 (Pairwise fine-tuning)**：加重「重要但被排後」的 pair 損失，讓系統重新學習排序。  
  - **單調校正 (Isotonic/等位數映射)**：重新校正分數刻度，讓系統分數更貼近專家等級。  
  - **輕量重排 (Rerank Top-M)**：只在前 M 位候選中做小幅調整。  

---

## 總結
完整流程：
1. **收集專家評級**  
2. **檢查一致性 (κ/α/W)**  
3. **若一致 → 聚合成共識排名**  
4. **與系統排名比較 (τ-b, Somers’ D, JT, NDCG 等)**  
5. **若不一致 → 逐專家分析 / 仲裁 / 機率模型**  
6. **持續改進演算法（誤差分析 + 微調/校正）**

這樣可同時確保 **專家意見可信**、**系統表現有量化依據**，並為後續演算法更新提供明確方向。


🔑 可以再做的事

1. 偏好式標記 (Pairwise / Preference Feedback)
	•	不一定要專家為每個候選人打分數（1–4 分），也可以用比較的方式：
「對這個題目，A 比 B 更合適嗎？」
	•	好處：專家更容易回答，負擔小；資料可以用來訓練 Learning-to-Rank 模型。

⸻

2. 專家一致性 / 可信度分析
	•	同一題給多位專家標記，檢查 一致性 (inter-rater agreement)：
	•	Cohen’s kappa、Krippendorff’s alpha
	•	若一致性低，代表題目本身有模糊性，系統也不容易排準。
	•	可以把一致性當作「題目難度」的 proxy。

⸻

3. 錯誤案例訪談 (Qualitative Study)
	•	挑出模型與專家分歧最大的題目，請專家說明原因：
	•	系統忽略了什麼特徵？（例如：跨領域背景）
	•	題目描述有歧義？
	•	這些質性意見可以直接指導下一步特徵工程或模型改進。

⸻

4. 分領域檢查
	•	不同領域可能標記量足夠 / 不足。
	•	可以檢查：
	•	哪些領域容易達成高 NDCG？
	•	哪些領域即使專家標了，系統仍然差距大？
	•	結果能幫助你判斷「需要針對哪些領域做特化模型」。

⸻

5. 多輪互動 (Iterative Labeling)
	•	不是一次把所有推薦名單丟給專家，而是 逐輪互動：
	•	系統先出 Top-10 → 專家標記 → 模型更新
	•	再產生新 Top-10 → 專家再標
	•	每輪都能檢查改進幅度，像 active learning 一樣收斂。

⸻

6. 時間成本 / 可用性分析
	•	除了看準確度，也可以記錄專家 標記一份題目所需的時間。
	•	若系統排序越好，專家標記時間應該會更短。
	•	這是一個「人因工程」角度的評估：不只是準確，也要節省專家工作量。

