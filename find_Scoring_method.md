# 候選人推薦評



---


## 二、演算法(離線)評估流程（含 Bootstrap + Top-K）
##2.1 目的

本章目標是建立一套系統化的評估方法，用於檢驗候選人推薦演算法的合理性、穩定性與實用性。
	•	合理性：透過 g-th Relevant Rank 與 g-Success@K 等指標，檢查推薦清單中歷史參考委員的出現位置，確保結果不偏離題目語意。
	•	穩定性：透過變異數與 Bootstrap 信賴區間，量化模型輸出在不同題目集合下的波動程度。
	•	實用性：結合多個 K 值設定（K-grid），觀察不同寬鬆程度下的覆蓋表現，為 UI 顯示範圍與實務流程提供依據。

---
##2.2基本敘述統計
查看前k個scores平均、變異數、分布，也可以依照學門來計算。

### 2.3 指標定義
對第 $i$ 題，系統輸出前 $K$ 名 $P_i@K$，已知委員集合為 $G_i$：

Success@K 曲線：  
  $\frac{1}{N}\sum_{i=1}^N \mathbf{I}（K^{(hit_g)}_i \le K)$  
  👉 白話：在所有題目裡，有多少比例能在前 $K$ 名內找到至少g個正確委員。這條曲線能幫助決定系統要顯示前幾名，才能涵蓋大部分正確答案。
g-th Relevant Rank:
 👉 計算第g位出現在候選人名單第幾位。
此兩個指標能藉由分佈、平均數、變異數推敲出此演算法狀況
例如：第一位出現位置很後面但變異數很小可能代表此演算法是穩定的，變異數很大可能代表此演算法是不穩定的諸如此類，需看結果而推斷，
此外可以與scores作參考來推斷。


---



### 2.4 Bootstrap（pro版，只能輸入固定題目）
## 簡單介紹：為什麼需要 Bootstrap？
在統計或機器學習裡，我們常常想知道：
- 我們手上的樣本有限，結果到底「穩不穩」？

👉 **藉由Boostrap從有限觀測數據中估計母體情況和其信賴區間**。  
它的好處是 **不用額外數學假設（例如常態分布）**，只要有電腦就能跑。
**目的**：反映「如果題目集合不同，整體平均指標會怎麼變」，進而給 **CI**。

（可參考: https://zhuanlan.zhihu.com/p/690904510 /）

**步驟**
1. 設定 Bootstrap 次數為`B`與亂數種子；令測試題目數為 `N`。
2. **預先快取** 每題在各 K 的 per-query 指標（加速）。
3. 進入迴圈 `b = 1..B`：
   - 從題目索引 `1..N` **有放回地抽樣 N 次**，得到 `idx_b`。
   - 對每個 `K ∈ K_grid`，在 `idx_b` 上取該指標的宏平均：
     - `相關指標[b,K]    = mean(指標[i,K]    for i in idx_b)`

   - 另外記錄 `q50_Khit[b]`、`q90_Khit[b]`（在 `idx_b` 的 \(K^{(hit)}\) 中位數、90 分位）。
4. 最後對每個 K：
   - **各個指標基本統計量**
   - **各個指標95% CI**：`percentile(mu_recall[:,K], [2.5, 97.5])`（其餘指標同理）
---

### 2.5 seek版雙層評估：描述/關鍵字穩健性
**目的**：若同一題有多個「描述/關鍵字」變體，評估對輸出的敏感度。

- **內層（描述層）**：每題建立 `DescPool_i`（m≈10–30：標準摘要、短描述、關鍵字串）。  
  每次在題目 \(i\) 上 **有放回抽 r=5–10 個描述**，對此題先取各指標的**平均**（降低單次描述的偶然性）。
- **外層（題目層）**：同 3.5 的 Bootstrap；只是每題的值改用「該題描述層平均」。


---
### 2.6 K 值選取（K-grid）
在推薦系統評估中，單一的 $K$ 往往不足以反映不同場景的需求。因此，我們同時計算一組 **Top-K 值集合**（稱為 K-grid），例如：  
$$
K \in \{ 10, 20, 30, 50...\}
$$

這樣做的目的：
1. **小 K（如  10）**：模擬「審查名單只能選很少人」的情境，要求系統在最前幾名就能給出正確人選。  
2. **大 K（如 30, 50）**：模擬「審查名單可以拉長」的情境，允許在較多候選裡面找到正確人選。  
3. **完整曲線觀察**：透過多個 K 的結果，可以畫出相關指標隨 K 變化的曲線，觀察系統的「隨寬鬆度而變化的表現」。  

報告會對每個 K 分別給出：  
- 指標平均值、變異數、分佈  
- 95% 信賴區間（由 Bootstrap 計算）  

---
### 2.7 分領域與總體報告
為了更細緻地分析效果，評估會同時產出：
1. **分領域 (per-subject)**：  
   - 對每個 subject / 學科領域分別計算 
   - 可看出系統在哪些領域表現好、哪些較弱
   - 例如：醫學領域命中率 85%，工程領域命中率 70%

2. **總體 (overall)**：  
   - 對所有題目整體取宏平均，得到全域效能分數  
   - 作為系統的總體指標，便於版本比較

這樣可以同時回答：  
- 系統在「總體」上的表現如何？  
- 系統在「不同 subject」之間是否有偏差？（公平性、泛化能力）
3. **試著把學門拿掉，看看與有分領域結果如何**：
  - 會不會推薦scores更高或更低？
  - 不同領域出現情況？👉相關指標：熵 (Entropy)
  - 
  



---
###  產出

**(A) 指標彙總表（Overall）**
- 針對每個 K（如 5/10/20/30/50），回報下列指標的「平均值 ± 95% CI」：
  - g-th Relevant Rank、Success@K
- 另回報：$K^{(hit)}$ 的 **median / p90 ± 95% CI**

**(B) 分領域表（Per-Subject）**
- 各 subject（學科/領域）分別產出與 (A) 同樣欄位，便於檢查偏科與泛化能力。
- 可附加「領域樣本數」與「權重占比」。

**(C) 圖形化結果**
- Success@K 曲線（含 95% CI 帶）：觀察 K 由小到大覆蓋率如何提升。
- 指標 vs K 折線圖（g-th Relevant Rank、Success@K，各自一條，含 CI）。
- $K^{(hit)}$ 分布圖（直方圖或 CDF）：觀察「第一次命中」通常落在哪個順位。

**(D) 覆蓋拆解（召回 vs 排序）**
- 候選池覆蓋率（粗召回是否把正解抓進來）。
- Top-K 命中率 / Success@K（排序是否把正解排到前面）。
- 目的：定位效能瓶頸是「召回不足」或「排序不佳」。

**(E) 可信度與可重現性**
- 報告 Bootstrap 參數：B 次數、亂數種子、K-grid、是否分層抽樣。
- 附上「平均、標準差、95% CI」三者，避免只報單一數字。
- 註明資料切分與時間切點（避免洩漏）。

**(F) 錯誤/案例分析（精簡版）**
- Top-K 失敗案例（如 Success@30=0 的題目）：列出題目、預期委員、模型前 K 結果（遮掩敏感資訊）。
- 常見失敗型態：同名歧義、冷門領域字彙、描述過短/過長等。

**(G) 決策建議**
- 依 $K^{(hit)}$ 的 **median / p90**，建議：
  - 介面預設顯示前 K（例：若 p90=40 → 預設顯示 40）。
  - 召回池寬度（例：至少召回 300，排序到前 50 供人工瀏覽）。
- 若某些 subject 顯著偏低，提出下一步改善（詞庫擴張、召回擴寬等）。
 --- 
### 2.10 一些想法（後面階段的）
- 確定seek版固定題目下稍微更改題目對於結果是否有顯著影響
- 因委員只能選五個人，是否本演算法所推薦得其實較佳
- 題目名字太過普遍是否影響結果
- 在測試資料中，能否有辦法事先標記額外委員也是合適的，以讓本演算法更準確
- 人工排查已確定委員是否真的重要和確定退件名單前幾名是極合適的
- 依照推薦分數區間分成群體
- 藉由使用者標記行為，來更新排名

Tier 2：人機互動評估（衡量品質）

這一層是衡量真正推薦品質的關鍵，也是上線前產生更可靠「金標」的方法。

方法：
對一組真實研究計畫產生 Top-N 推薦清單。
組成領域專家小組（理想為真實或前任召集人）。
向專家呈現計畫與匿名化的推薦名單（不顯示分數或排名）。
請專家依相關度評分（例如：1 = 無關、2 = 牽強、3 = 相關、4 = 高度相關）。
好處：
產生高品質標註資料：專家評分可成為更可靠的金標。
支援更合適的指標：有了分級的相關性分數後，可使用 NDCG@K（歸一化折損累積增益），能獎勵把高度相關項目排在前面，符合我們需求。
定性回饋：可收集專家對評分原因的質性意見，補足量化指標的不足。
注意：這不是純量化任務，人的判斷很重要。我會協助一起執行人機評估。

Tier 3：線上 A/B 測試（衡量影響）

這是衡量系統對真實使用者行為影響的最終測試，與 Netflix／Spotify 類似。

方法：
在真實召集人群體中部署推薦系統。
完整紀錄使用者互動行為。
當推出新版本演算法時，將使用者分流到 B 組（新演算法）與 A 組（現有版本）進行比較。
主要指標：
採納率（Adoption Rate）：被召集人最終採納為委員的推薦候選人比例——這是最重要的指標，相當於電商的「購買」。
點擊率（CTR）：被點開檔案的推薦候選人比例，代表使用者興趣。
被採納候選人的平均排名：使用者採納的候選人平均位於推薦清單的哪裡？平均排名越低代表排序效果越好。
單次會話成功率：每次會話中至少採納一位推薦候選人的比例。

MAP（Mean Average Precision）： $\frac{1}{N}\sum_{i=1}^N \text{AP}_i$  
其中 $\text{AP}_i$ = 針對題目 $i$ 的 **平均精確度 (Average Precision)**，只在「命中位置」計算 Precision 再取平均。  
👉 白話：MAP 不只檢查有沒有命中，還會考慮「正確答案排得多前面」，越早出現貢獻越大。


NDCG@K：可採二元或分級相關性定義，依 ground truth 標註設計。
（可參考：https://yehjames.medium.com/python推薦系統-常見-線下-排序評估指標-90876d70a01 /）
 

