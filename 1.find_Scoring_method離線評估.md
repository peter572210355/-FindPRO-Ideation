# 候選人推薦評



---


## 二、演算法(離線)評估流程（含 Bootstrap + Top-K）
##2.1 目的

本章目標是建立一套系統化的評估方法，用於檢驗候選人推薦演算法的合理性、穩定性與實用性。

	•	合理性：透過 g-th Relevant Rank 與 g-Success@K 等指標，檢查推薦清單中歷史參考委員的出現位置，確保結果不偏離題目語意。
 
	•	穩定性：透過變異數與 Bootstrap 信賴區間，量化模型輸出在不同題目集合下的波動程度。
 
	•	實用性：結合多個 K 值設定（K-grid），觀察不同寬鬆程度下的覆蓋表現，為 UI 顯示範圍與實務流程提供依據。

---
##2.2基本敘述統計

查看前k個scores平均、變異數、分布，也可以依照學門來計算。

### 2.3 指標定義

對第 $i$ 題，系統輸出前 $K$ 名 $P_i@K$，已知委員集合為 $G_i$：


Success@K 曲線：  
  $\frac{1}{N}\sum_{i=1}^N \mathbf{I}（K^{(hit_g)}_i \le K)$  
  👉 白話：在所有題目裡，有多少比例能在前 $K$ 名內找到至少g個正確委員。這條曲線能幫助決定系統要顯示前幾名，才能涵蓋大部分正確答案。
  
g-th Relevant Rank:
 👉 計算第g位出現在候選人名單第幾位。
 
此兩個指標能藉由分佈、平均數、變異數推敲出此演算法狀況
例如：第一位出現位置很後面但變異數很小可能代表此演算法是穩定的，變異數很大可能代表此演算法是不穩定的諸如此類，需看結果而推斷，
此外可以與scores作參考來推斷。

# 多樣性
    ILD
	Category Diversity（熵）
	


---



### 2.4 Bootstrap（pro版，只能輸入固定題目）
## 簡單介紹：為什麼需要 Bootstrap？
在統計或機器學習裡，我們常常想知道：
- 我們手上的樣本有限，結果到底「穩不穩」？

👉 **藉由Boostrap從有限觀測數據中估計母體情況和其信賴區間**。  
它的好處是 **不用額外數學假設（例如常態分布）**，只要有電腦就能跑。
**目的**：反映「如果題目集合不同，整體平均指標會怎麼變」，進而給 **CI**。

（可參考: https://zhuanlan.zhihu.com/p/690904510 /）

**步驟**
1. 設定 Bootstrap 次數為`B`與亂數種子；令測試題目數為 `N`。
2. **預先快取** 每題在各 K 的 per-query 指標（加速）。
3. 進入迴圈 `b = 1..B`：
   - 從題目索引 `1..N` **有放回地抽樣 N 次**，得到 `idx_b`。
   - 對每個 `K ∈ K_grid`，在 `idx_b` 上取該指標的宏平均：
     - `相關指標[b,K]    = mean(指標[i,K]    for i in idx_b)`

   - 另外記錄 `q50_Khit[b]`、`q90_Khit[b]`（在 `idx_b` 的 \(K^{(hit)}\) 中位數、90 分位）。
4. 最後對每個 K：
   - **各個指標基本統計量**
   - **各個指標95% CI**：`percentile(mu_recall[:,K], [2.5, 97.5])`（其餘指標同理）
---

### 2.5 seek版雙層評估：描述/關鍵字穩健性
**目的**：若同一題有多個「描述/關鍵字」變體，評估對輸出的敏感度。

- **內層（描述層）**：每題建立 `DescPool_i`（m≈10–30：標準摘要、短描述、關鍵字串）。  
  每次在題目 \(i\) 上 **有放回抽 r=5–10 個描述**，對此題先取各指標的**平均**（降低單次描述的偶然性）。
- **外層（題目層）**：同 3.5 的 Bootstrap；只是每題的值改用「該題描述層平均」。


---
### 2.6 K 值選取（K-grid）
在推薦系統評估中，單一的 $K$ 往往不足以反映不同場景的需求。因此，我們同時計算一組 **Top-K 值集合**（稱為 K-grid），例如：  
$$
K \in \{ 10, 20, 30, 50...\}
$$

這樣做的目的：
1. **小 K（如  10）**：模擬「審查名單只能選很少人」的情境，要求系統在最前幾名就能給出正確人選。  
2. **大 K（如 30, 50）**：模擬「審查名單可以拉長」的情境，允許在較多候選裡面找到正確人選。  
3. **完整曲線觀察**：透過多個 K 的結果，可以畫出相關指標隨 K 變化的曲線，觀察系統的「隨寬鬆度而變化的表現」。  

報告會對每個 K 分別給出：  
- 指標平均值、變異數、分佈  
- 95% 信賴區間（由 Bootstrap 計算）  

---
### 2.7 分領域與總體報告
為了更細緻地分析效果，評估會同時產出：
1. **分領域 (per-subject)**：  
   - 對每個 subject / 學科領域分別計算 
   - 可看出系統在哪些領域表現好、哪些較弱
   - 例如：醫學領域命中率 85%，工程領域命中率 70%

2. **總體 (overall)**：  
   - 對所有題目整體取宏平均，得到全域效能分數  
   - 作為系統的總體指標，便於版本比較

這樣可以同時回答：  
- 系統在「總體」上的表現如何？  
- 系統在「不同 subject」之間是否有偏差？（公平性、泛化能力）
3. **試著把學門拿掉，看看與有分領域結果如何**：
  - 會不會推薦scores更高或更低？
  - 不同領域出現情況？👉相關指標：熵 (Entropy)
  - 
  



---
###  產出

**(A) 指標彙總表（Overall）**
- 針對每個 K（如 5/10/20/30/50），回報下列指標的「平均值 ± 95% CI」：
  - g-th Relevant Rank、Success@K
- 另回報：$K^{(hit)}$ 的 **median / p90 ± 95% CI**

**(B) 分領域表（Per-Subject）**
- 各 subject（學科/領域）分別產出與 (A) 同樣欄位，便於檢查偏科與泛化能力。
- 可附加「領域樣本數」與「權重占比」。

**(C) 圖形化結果**
- Success@K 曲線（含 95% CI 帶）：觀察 K 由小到大覆蓋率如何提升。
- 指標 vs K 折線圖（g-th Relevant Rank、Success@K，各自一條，含 CI）。
- $K^{(hit)}$ 分布圖（直方圖或 CDF）：觀察「第一次命中」通常落在哪個順位。

**(D) 覆蓋拆解（召回 vs 排序）**
- 候選池覆蓋率（粗召回是否把正解抓進來）。
- Top-K 命中率 / Success@K（排序是否把正解排到前面）。
- 目的：定位效能瓶頸是「召回不足」或「排序不佳」。

**(E) 可信度與可重現性**
- 報告 Bootstrap 參數：B 次數、亂數種子、K-grid、是否分層抽樣。
- 附上「平均、標準差、95% CI」三者，避免只報單一數字。
- 註明資料切分與時間切點（避免洩漏）。

**(F) 錯誤/案例分析（精簡版）**
- Top-K 失敗案例（如 Success@30=0 的題目）：列出題目、預期委員、模型前 K 結果（遮掩敏感資訊）。
- 常見失敗型態：同名歧義、冷門領域字彙、描述過短/過長等。

**(G) 決策建議**
- 依 $K^{(hit)}$ 的 **median / p90**，建議：
  - 介面預設顯示前 K（例：若 p90=40 → 預設顯示 40）。
  - 召回池寬度（例：至少召回 300，排序到前 50 供人工瀏覽）。
- 若某些 subject 顯著偏低，提出下一步改善（詞庫擴張、召回擴寬等）。
 --- 
# 想法
或許 不分領域 跟設領域 可以混合用 離線評估後計算需要多少專家多少題目
